{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap06/adam_model.ipynb\n",
    "go_=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnBasicModel(AdamModel):\n",
    "    def __init__(self, name, dataset, hconfigs, show_maps = False):\n",
    "        if isinstance(hconfigs, list) and \\\n",
    "        not isinstance(hconfigs[0], (list, int)):\n",
    "            hconfigs = [hconfigs]\n",
    "        self.show_maps = show_maps\n",
    "        self.need_maps = False\n",
    "        self.kernels = []\n",
    "        super(CnnBasicModel, self).__init__(name, dataset, hconfigs)\n",
    "        self.use_adam = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_alloc_layer_param(self, input_shape, hconfig):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    m_name = 'alloc_{}_layer'.format(layer_type)\n",
    "    method = getattr(self, m_name)\n",
    "    pm, output_shape = method(input_shape, hconfig)\n",
    "\n",
    "    return pm, output_shape\n",
    "\n",
    "CnnBasicModel.alloc_layer_param = cnn_basic_alloc_layer_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_layer(self, x, hconfig, pm):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    m_name = 'forward_{}_layer'.format(layer_type)\n",
    "    method = getattr(self, m_name)\n",
    "    y, aux = method(x, hconfig, pm)\n",
    "        \n",
    "    return y, aux\n",
    "\n",
    "CnnBasicModel.forward_layer = cnn_basic_forward_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_backprop_layer(self, G_y, hconfig, pm, aux):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    m_name = 'backprop_{}_layer'.format(layer_type)\n",
    "    method = getattr(self, m_name)\n",
    "    G_input = method(G_y, hconfig, pm, aux)\n",
    "\n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.backprop_layer = cnn_basic_backprop_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_alloc_conca_layer(self, input_shape, hconfig):\n",
    "    if self.jongwoo==1:\n",
    "        input_cnt = np_cpu.prod(input_shape)+go_\n",
    "        self.jong_input_cnt=input_cnt\n",
    "        \n",
    "    else:\n",
    "        input_cnt = np_cpu.prod(input_shape)\n",
    "    output_cnt = get_conf_param(hconfig, 'width', hconfig)\n",
    "\n",
    "    weight = np.random.normal(0, self.rand_std, [input_cnt, output_cnt], dtype = 'float32')\n",
    "    weight[-go_:][:]=1*weight[-go_:][:]\n",
    "    #print(\"conca\",weight.shape)\n",
    "    bias = np.zeros([output_cnt], dtype = 'float32')\n",
    "    #special = np.random.normal(20, self.rand_std, [30])\n",
    "    return {'w':weight, 'b':bias}, [output_cnt]\n",
    "    #return {'w':weight, 'b':bias,'s':special}, [output_cnt]\n",
    "def cnn_basic_alloc_full_layer(self, input_shape, hconfig):\n",
    "    input_cnt = np_cpu.prod(input_shape)   #입력층의 크기가 얼마인가?\n",
    "    output_cnt = get_conf_param(hconfig, 'width', hconfig) #출력 층의 크기가 얼마인가?\n",
    "\n",
    "    weight = np.random.normal(0, self.rand_std, [input_cnt, output_cnt], dtype = 'float32')  #2차원 행렬 [입력층 크기, 출력 층 크기] \n",
    "                                                                                            #무작위 값을 생성하라.\n",
    "    bias = np.zeros([output_cnt], dtype = 'float32')          #바이어스 초기값 0으로 생성하라.\n",
    "    #print(\"full\",weight.shape)\n",
    "    return {'w':weight, 'b':bias}, [output_cnt]    #생성된 가중치 전달해주자.\n",
    "    \n",
    "def cnn_basic_alloc_conv_layer(self, input_shape, hconfig):\n",
    "    assert len(input_shape) == 3\n",
    "    xh, xw, xchn = input_shape\n",
    "    kh, kw = get_conf_param_2d(hconfig, 'ksize')\n",
    "    ychn = get_conf_param(hconfig, 'chn')\n",
    "\n",
    "    kernel = np.random.normal(0, self.rand_std, [kh, kw, xchn, ychn], dtype = 'float32')\n",
    "    bias = np.zeros([ychn], dtype = 'float32')\n",
    "\n",
    "    if self.show_maps: self.kernels.append(kernel)\n",
    "\n",
    "    return {'k':kernel, 'b':bias}, [xh, xw, ychn]\n",
    "    \n",
    "def cnn_basic_alloc_pool_layer(self, input_shape, hconfig):\n",
    "    assert len(input_shape) == 3\n",
    "    xh, xw, xchn = input_shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "\n",
    "    assert xh % sh == 0\n",
    "    assert xw % sw == 0\n",
    "\n",
    "    return {}, [xh//sh, xw//sw, xchn]\n",
    "CnnBasicModel.alloc_conca_layer = cnn_basic_alloc_conca_layer\n",
    "CnnBasicModel.alloc_full_layer = cnn_basic_alloc_full_layer\n",
    "CnnBasicModel.alloc_conv_layer = cnn_basic_alloc_conv_layer\n",
    "CnnBasicModel.alloc_max_layer = cnn_basic_alloc_pool_layer\n",
    "CnnBasicModel.alloc_avg_layer = cnn_basic_alloc_pool_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_alloc_ele_conca_layer(self, input_shape, hconfig):\n",
    "    if self.jongwoo==1:\n",
    "        input_cnt = np_cpu.prod(input_shape)+go_    #go_ 은 1차원 크로마틴 정보 노드의 크기입니다. 막상 보니 필요없는 코드였네요.\n",
    "        self.jong_input_cnt=input_cnt\n",
    "\n",
    "    output_cnt = get_conf_param(hconfig, 'width', hconfig) #출력 크기는 40 입니다.\n",
    "    #print(output_cnt)\n",
    "    weight = np.random.normal(0, self.rand_std, [go_, output_cnt], dtype = 'float32') # 2차원 행렬인데 크기 [1, 40] 가중치를 만듭니다.\n",
    "\n",
    "    #print(\"conca\",weight.shape)\n",
    "    #bias = np.zeros([output_cnt], dtype = 'float32')\n",
    "    #special = np.random.normal(20, self.rand_std, [30])\n",
    "    return {'w':weight}, [output_cnt]   # 바이어스 했다는 내용도 없어 생략했습니다. 가중치만 전달해줍니다.\n",
    "    #return {'w':weight, 'b':bias,'s':special}, [output_cnt]\n",
    "\n",
    "\n",
    "def cnn_basic_forward_ele_conca_layer(self, x, hconfig, pm): # 포워딩 함수입니다.\n",
    "    if pm is None: return x, None\n",
    "    \n",
    "    x_org_shape = x.shape\n",
    "    #print(x.shape)\n",
    "    if len(x.shape) != 2:  #2차원이 아닌 출력 값들을 펴주는 역할입니다.\n",
    "        mb_size = x.shape[0]\n",
    "        x = x.reshape([mb_size, -1])\n",
    "    if self.jongwoo == 1:\n",
    "        if self.is_training==True:  #학습할 때\n",
    "            from_idx_ = self.n * self.batch_size  #현재 입력 값에 대한 크로마틴 정보 번호를 찾고 있습니다.\n",
    "            to_idx_ = (self.n + 1) * self.batch_size\n",
    "            temp_temp=self.dataset.ext_training_xs[self.dataset.indices[from_idx_:to_idx_]]  #[배치사이즈, 1] 형태로 값을 얻습니다.\n",
    "            temp_temp=temp_temp.reshape([-1, go_])\n",
    "            \n",
    "        else:    #테스트할 때\n",
    "            temp_temp=self.dataset.ext_test_xs   #테스트 값에 대한 크로마틴 정보 출력합니다.\n",
    "            #next_temp=temp_temp.reshape([-1, go_])\n",
    "            temp_temp=temp_temp.reshape([-1, go_])\n",
    "    \n",
    "    affine = np.matmul(temp_temp, pm['w'])  #크로마틴 정보와 가중치가 연산을 진행합니다.\n",
    "    #y = self.activate(affine, hconfig)\n",
    "    y = np.multiply(x, affine)   #이후 원래 입력 값인x와 element-wise 연산을 수행합니다.\n",
    "    #print(\"output\",y.shape)\n",
    "    return y, [x, y, x_org_shape, affine, temp_temp]  #최종 값 y와 역전파에 필요한 정보들을 회수해갑니다.\n",
    "\n",
    "def cnn_basic_backprop_ele_conca_layer(self, G_y, hconfig, pm, aux): # 각종 필요한 값들을 받습니다.\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape, affine_, temp_temp = aux  #역전파에 필요한 정보 그대로 수령합니다.\n",
    "    \n",
    "    #G_affine = self.activate_derv(G_y, y, hconfig)\n",
    "    \n",
    "    G_affine = G_y \n",
    "    G_affine = G_affine*affine_ # for x      # 최후에 연산된 것은 기존 x 값과 크로마틴 40차원 벡터의 곱 연산입니다.\n",
    "                                             # 여기서 특징은 element-wise 연산이므로, 역전파 공식에 의해 단순한 곱셈으로 처리됩니다.\n",
    "                                             # 행렬 곱이 사용안된 것이 특징입니다.\n",
    "    G_affine_extra = G_affine*x              # 40차원으로 표현된 값에 해당하는 에러도 마찬가지로 element-wise로 계산되었기 때문에\n",
    "                                             # x를 곱해주면서 처리됩니다.\n",
    "    #print(G_affine.shape,G_affine_extra.shape)\n",
    "    #g_affine_weight = x.transpose()\n",
    "    #print(g_affine_weight.shape)\n",
    "    g_affine_weight = temp_temp.transpose()  #여기서 사용된건 fully connected layer 연산이므로 크로마틴 값을 전치하여 계산준비합니다.\n",
    "    #print(g_affine_weight.shape,G_affine_extra.shape)\n",
    "    G_weight = np.matmul(g_affine_weight, G_affine_extra) #크로마틴을 40차원으로 변형해주는 가중치에 해당하는 에러를 계산합니다.\n",
    "    #G_bias = np.sum(G_affine, axis = 0)\n",
    "    #G_input = np.matmul(G_affine, g_affine_input)\n",
    "    \n",
    "    self.update_param(pm, 'w', G_weight)  #가중치 업데이트 해줍니다.\n",
    "    \n",
    "    \n",
    "    return G_affine.reshape(x_org_shape)   #출력 원형으로 복구해주고 아래로 에러를 전달해줍니다.\n",
    "\n",
    "CnnBasicModel.backprop_ele_conca_layer = cnn_basic_backprop_ele_conca_layer\n",
    "\n",
    "CnnBasicModel.forward_ele_conca_layer = cnn_basic_forward_ele_conca_layer\n",
    "\n",
    "CnnBasicModel.alloc_ele_conca_layer = cnn_basic_alloc_ele_conca_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_type(hconfig):\n",
    "    if not isinstance(hconfig, list): return 'full'\n",
    "    return hconfig[0]\n",
    "\n",
    "def get_conf_param(hconfig, key, defval = None):\n",
    "    if not isinstance(hconfig, list): return defval #리스트가 없으면 return\n",
    "    if len(hconfig) <= 1: return defval\n",
    "    if not key in hconfig[1]: return defval\n",
    "    return hconfig[1][key]\n",
    "    \n",
    "def get_conf_param_2d(hconfig, key, defval = None):\n",
    "    if len(hconfig) <= 1: return defval\n",
    "    if not key in hconfig[1]: return defval\n",
    "    val = hconfig[1][key]\n",
    "    if isinstance(val, list): return val\n",
    "    return [val, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def cnn_basic_forward_conca_layer(self, x, hconfig, pm):\n",
    "    if pm is None: return x, None\n",
    "    \n",
    "    x_org_shape = x.shape\n",
    "    #print(x.shape)\n",
    "    if len(x.shape) != 2:\n",
    "        mb_size = x.shape[0]\n",
    "        x = x.reshape([mb_size, -1])\n",
    "    if self.jongwoo == 1:\n",
    "        if self.is_training==True:\n",
    "            from_idx_ = self.n * self.batch_size\n",
    "            to_idx_ = (self.n + 1) * self.batch_size\n",
    "            temp_temp=self.dataset.ext_training_xs[self.dataset.indices[from_idx_:to_idx_]]\n",
    "            next_temp=temp_temp.reshape([-1, go_])\n",
    "            #temp_temp=temp_temp.reshape([-1, go_])\n",
    "            #temp_temp=next_temp[:,-go_:]\n",
    "            #print(temp_temp.shape,\"temp_temp.shape\")\n",
    "            #\n",
    "            #print(pm['s'])\n",
    "            temp_temp=next_temp*pm['s']\n",
    "            #print(x.shape, temp_temp.shape)\n",
    "            x=np.concatenate((x,temp_temp),axis=1)\n",
    "        else:\n",
    "            temp_temp=self.dataset.ext_test_xs\n",
    "            next_temp=temp_temp.reshape([-1, go_])\n",
    "            #temp_temp=temp_temp.reshape([-1, go_])\n",
    "            #next_temp=temp_temp\n",
    "            temp_temp=next_temp*pm['s']\n",
    "            x=np.concatenate((x,temp_temp),axis=1)\n",
    "      \n",
    "        #print(\"x.shape\",x.shape)\n",
    "    #-----------\n",
    "    \n",
    "    #------------------\n",
    "    affine = np.matmul(x, pm['w']) + pm['b']\n",
    "    y = self.activate(affine, hconfig)\n",
    "    \n",
    "    return y, [x, y, x_org_shape, next_temp]\n",
    "\n",
    "CnnBasicModel.forward_conca_layer = cnn_basic_forward_conca_layer\n",
    "\"\"\"\n",
    "def cnn_basic_forward_conca_layer(self, x, hconfig, pm):\n",
    "    if pm is None: return x, None\n",
    "    \n",
    "    x_org_shape = x.shape\n",
    "    #print(x.shape)\n",
    "    if len(x.shape) != 2:\n",
    "        mb_size = x.shape[0]\n",
    "        x = x.reshape([mb_size, -1])\n",
    "    if self.jongwoo == 1:\n",
    "        if self.is_training==True:\n",
    "            from_idx_ = self.n * self.batch_size\n",
    "            to_idx_ = (self.n + 1) * self.batch_size\n",
    "            temp_temp=self.dataset.ext_training_xs[self.dataset.indices[from_idx_:to_idx_]]\n",
    "            #next_temp=temp_temp.reshape([-1, go_])\n",
    "            temp_temp=temp_temp.reshape([-1, go_])\n",
    "            #temp_temp=next_temp[:,-go_:]\n",
    "            #print(temp_temp.shape,\"temp_temp.shape\")\n",
    "            #\n",
    "            #print(pm['s'])\n",
    "            #temp_temp=next_temp*pm['s']\n",
    "            #print(x.shape, temp_temp.shape)\n",
    "            x=np.concatenate((x,temp_temp),axis=1)\n",
    "        else:\n",
    "            temp_temp=self.dataset.ext_test_xs\n",
    "            #next_temp=temp_temp.reshape([-1, go_])\n",
    "            temp_temp=temp_temp.reshape([-1, go_])\n",
    "            #next_temp=temp_temp\n",
    "            #temp_temp=next_temp*pm['s']\n",
    "            x=np.concatenate((x,temp_temp),axis=1)\n",
    "      \n",
    "        #print(\"x.shape\",x.shape)\n",
    "    #-----------\n",
    "    \n",
    "    #------------------\n",
    "    affine = np.matmul(x, pm['w']) + pm['b']\n",
    "    y = self.activate(affine, hconfig)\n",
    "    \n",
    "    return y, [x, y, x_org_shape]\n",
    "\n",
    "CnnBasicModel.forward_conca_layer = cnn_basic_forward_conca_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def cnn_basic_backprop_conca_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape= aux\n",
    "    \n",
    "    G_affine = self.activate_derv(G_y, y, hconfig)\n",
    "    \n",
    "    \n",
    "    g_affine_weight = x.transpose()\n",
    "    g_affine_input = pm['w'].transpose()\n",
    "    \n",
    "    G_weight = np.matmul(g_affine_weight, G_affine) \n",
    "    G_bias = np.sum(G_affine, axis = 0)\n",
    "    G_input = np.matmul(G_affine, g_affine_input)\n",
    "    if self.epoch<140:\n",
    "        self.inter_stop_1=1\n",
    "        self.update_param(pm, 'w', G_weight)\n",
    "        self.update_param(pm, 'b', G_bias)\n",
    "        self.inter_stop_1=0\n",
    "\n",
    "        \n",
    "    else:\n",
    "        self.inter_stop=1\n",
    "        self.update_param(pm, 'w', G_weight)\n",
    "        self.update_param(pm, 'b', G_bias)\n",
    "        self.inter_stop=0\n",
    "        self.stop=1\n",
    "    #print(G_affine.shape)\n",
    "    #print(g_affine_weight.shape)\n",
    "    #print(g_affine_input.shape)\n",
    "    #print(G_weight.shape)\n",
    "    #print(G_bias.shape)\n",
    "    #print(G_input.shape)\n",
    "    if self.jongwoo == 1:\n",
    "        \n",
    "        return G_input[:,:-go_].reshape(x_org_shape)\n",
    "    else:\n",
    "        return G_input.reshape(x_org_shape)\n",
    "\n",
    "CnnBasicModel.backprop_conca_layer = cnn_basic_backprop_conca_layer\n",
    "\n",
    "def cnn_basic_backprop_conca_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape,next_temp = aux\n",
    "    \n",
    "    G_affine = self.activate_derv(G_y, y, hconfig)\n",
    "    \n",
    "    \n",
    "    g_affine_weight = x.transpose()\n",
    "    \n",
    "    #g_affine_weight = g_affine_weight[-go_:,:]*pm['s']\n",
    "    g_affine_input = pm['w'].transpose()\n",
    "    \n",
    "    G_weight = np.matmul(g_affine_weight, G_affine) \n",
    "    G_bias = np.sum(G_affine, axis = 0)\n",
    "    G_input = np.matmul(G_affine, g_affine_input)\n",
    "    \n",
    "    G_temp=G_input[:,-go_:]\n",
    "    G_temp_1=next_temp*G_temp\n",
    "    G_stair = np.sum(G_temp_1,axis=0)\n",
    "    \n",
    "    #G_stair = np.sum(G_weight\n",
    "    #G_stair = np.sum(G_input[:,-go_:]*x[:,-go_:], axis = 0) \n",
    "    #print(G_stair)\n",
    "    self.update_param(pm, 'w', G_weight)\n",
    "    self.update_param(pm, 'b', G_bias)\n",
    "    self.learning_rate*=1\n",
    "    self.update_param(pm, 's', G_stair)\n",
    "    self.learning_rate/=1\n",
    "    #print(G_affine.shape)\n",
    "    #print(g_affine_weight.shape)\n",
    "    #print(g_affine_input.shape)\n",
    "    #print(G_weight.shape)\n",
    "    #print(G_bias.shape)\n",
    "    #print(G_input.shape)\n",
    "    if self.jongwoo == 1:\n",
    "        \n",
    "        return G_input[:,:-go_].reshape(x_org_shape)\n",
    "    else:\n",
    "        return G_input.reshape(x_org_shape)\n",
    "\n",
    "CnnBasicModel.backprop_conca_layer = cnn_basic_backprop_conca_layer\n",
    "\"\"\"\n",
    "def cnn_basic_backprop_conca_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape = aux\n",
    "    \n",
    "    G_affine = self.activate_derv(G_y, y, hconfig)\n",
    "    \n",
    "    G_affine = G_affine \n",
    "    g_affine_weight = x.transpose()\n",
    "    g_affine_input = pm['w'].transpose()\n",
    "    \n",
    "    G_weight = np.matmul(g_affine_weight, G_affine) \n",
    "    G_bias = np.sum(G_affine, axis = 0)\n",
    "    G_input = np.matmul(G_affine, g_affine_input)\n",
    "    #G_stair = np.zeros(go_)\n",
    "    #for i in range(go_):\n",
    "    #    G_stair[i] = np.sum(x[:,-i]*G_weight[-i,:],axis=0)\n",
    "    \n",
    "    self.update_param(pm, 'w', G_weight)\n",
    "    self.update_param(pm, 'b', G_bias)\n",
    "    #self.update_param(pm, 's', G_stair)\n",
    "    #print(G_affine.shape)\n",
    "    #print(g_affine_weight.shape)\n",
    "    #print(g_affine_input.shape)\n",
    "    #print(G_weight.shape)\n",
    "    #print(G_bias.shape)\n",
    "    #print(G_input.shape)\n",
    "    if self.jongwoo == 1:\n",
    "        \n",
    "        return G_input[:,:-go_].reshape(x_org_shape)\n",
    "    else:\n",
    "        return G_input.reshape(x_org_shape)\n",
    "\n",
    "CnnBasicModel.backprop_conca_layer = cnn_basic_backprop_conca_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_full_layer(self, x, hconfig, pm): # x 는 아래층에서 오는 입력 값, hconfig 는 해당 층의 메타 정보, pm은 가중치\n",
    "    if pm is None: return x, None\n",
    "    \n",
    "    x_org_shape = x.shape\n",
    "    \n",
    "    if len(x.shape) != 2: #convoultional layer 경우에는 출력 값이 아닙니다. 그러므로 fully connected layer를 위해 펼쳐줘야겠죠.\n",
    "        mb_size = x.shape[0]\n",
    "        x = x.reshape([mb_size, -1]) #펼쳐줍니다.\n",
    "        \n",
    "    affine = np.matmul(x, pm['w']) + pm['b'] #가장 기본적인 인공신경망 연산입니다. matmul은 행렬곱을 의미합니다.\n",
    "                                             #입력 값과 가중치를 행렬 곱을 하고 바이어스를 더해줍니다.\n",
    "    y = self.activate(affine, hconfig)       #활성함수에 넣어서 출력 결과를 얻습니다. 다른 곳에 모든 활성함수 일일히 정의하였습니다.\n",
    "    \n",
    "    return y, [x, y, x_org_shape]   #y를 다음 층으로 건내주고, 역전파에 필요한 x, y, 등 정보도 기록할겁니다.\n",
    "\n",
    "CnnBasicModel.forward_full_layer = cnn_basic_forward_full_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_backprop_full_layer(self, G_y, hconfig, pm, aux): # G_y는 윗 층에서 온 에러 정보, aux 는 역전파에 필요한 x, y 정보들입니다\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape = aux  #aux 에서 각 정보를 가지고 옵니다. 역전파 할 때는 순전파 할 떄 사용된 x와 y가 필요합니다.\n",
    "    \n",
    "    G_affine = self.activate_derv(G_y, y, hconfig)  # 역전파는 전부 반대 순서대로 합니다. 가장 나중에 사용된 연산인\n",
    "                                                    # 활성함수 연산이 만들어낸 오차를 G_y에 첨가해줍니다. \n",
    "    \n",
    "    g_affine_weight = x.transpose()    #가중치의 미분 값은 위에서 내려온 에러 곱하기 x의 전치행렬이므로 x. 전치행렬을 구합니다.\n",
    "    g_affine_input = pm['w'].transpose() #아래 층으로 전달될 에러 값은 위에서 내려온 에러 값 곱하기 가중치 전치행렬입니다.\n",
    "    \n",
    "    G_weight = np.matmul(g_affine_weight, G_affine)  #가중치에 해당하는 에러 크기를 계산하였습니다.\n",
    "    G_bias = np.sum(G_affine, axis = 0)   #바이어스는 모든 데이터에서 온 에러 값을 더해주기 위해 batch 차원인 0번 차원 기준으로 더합니다.\n",
    "    G_input = np.matmul(G_affine, g_affine_input)  #아래로 전달해줄 에러 계산을 해줍니다.\n",
    "    \n",
    "    self.update_param(pm, 'w', G_weight)  #가중치 업데이트 해줍니다.\n",
    "    self.update_param(pm, 'b', G_bias)    #바이어스 업데이트 해줍니다.\n",
    "\n",
    "    return G_input.reshape(x_org_shape)  #포워딩 때 해준 입력 층을 펼쳐주는걸 복구해주기 위해 다시 변형한 다음 전달해줍니다.\n",
    "\n",
    "CnnBasicModel.backprop_full_layer = cnn_basic_backprop_full_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_activate(self, affine, hconfig):\n",
    "    if hconfig is None: return affine\n",
    "    \n",
    "    func = get_conf_param(hconfig, 'actfunc', 'relu')\n",
    "    #print(func)\n",
    "    if func == 'none':      return affine\n",
    "    elif func == 'relu':    return relu(affine)\n",
    "    elif func == 'sigmoid': return sigmoid(affine)\n",
    "    elif func == 'tanh':    return tanh(affine)\n",
    "    else:                   assert 0\n",
    "        \n",
    "def cnn_basic_activate_derv(self, G_y, y, hconfig):\n",
    "    if hconfig is None: return G_y\n",
    "    \n",
    "    func = get_conf_param(hconfig, 'actfunc', 'relu')\n",
    "    \n",
    "    if func == 'none':      return G_y\n",
    "    elif func == 'relu':    return relu_derv(y) * G_y\n",
    "    elif func == 'sigmoid': return sigmoid_derv(y) * G_y\n",
    "    elif func == 'tanh':    return tanh_derv(y) * G_y\n",
    "    else:                   assert 0\n",
    "\n",
    "CnnBasicModel.activate = cnn_basic_activate\n",
    "CnnBasicModel.activate_derv = cnn_basic_activate_derv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_conv_layer_adhoc(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    kh, kw, _, ychn = pm['k'].shape\n",
    "    \n",
    "    conv = np.zeros((mb_size, xh, xw, ychn))\n",
    "    \n",
    "    for n in range(mb_size):\n",
    "        for r in range(xh):\n",
    "            for c in range(xw):\n",
    "                for ym in range(ychn):\n",
    "                    for i in range(kh):\n",
    "                        for j in range(kw):\n",
    "                            rx = r + i - (kh-1) // 2\n",
    "                            cx = c + j - (kw-1) // 2\n",
    "                            if rx < 0 or rx >= xh: continue\n",
    "                            if cx < 0 or cx >= xw: continue\n",
    "                            for xm in range(xchn):\n",
    "                                kval = pm['k'][i][j][xm][ym]\n",
    "                                ival = x[n][rx][cx][xm]\n",
    "                                conv[n][r][c][ym] += kval * ival\n",
    "\n",
    "    y = self.activate(conv + pm['b'], hconfig)\n",
    "    \n",
    "    return y, [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_conv_layer_better(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    kh, kw, _, ychn = pm['k'].shape\n",
    "    \n",
    "    conv = np.zeros((mb_size, xh, xw, ychn))\n",
    "\n",
    "    bh, bw = (kh-1)//2, (kw-1)//2\n",
    "    eh, ew = xh + kh - 1, xw + kw - 1\n",
    "    \n",
    "    x_ext = np.zeros((mb_size, eh, ew, xchn))\n",
    "    x_ext[:, bh:bh + xh, bw:bw + xw, :] = x\n",
    "    \n",
    "    k_flat = pm['k'].transpose([3, 0, 1, 2]).reshape([ychn, -1])\n",
    "    \n",
    "    for n in range(mb_size):\n",
    "        for r in range(xh):\n",
    "            for c in range(xw):\n",
    "                for ym in range(ychn):\n",
    "                    xe_flat = x_ext[n, r:r + kh, c:c + kw, :].flatten()\n",
    "                    conv[n, r, c, ym] = (xe_flat*k_flat[ym]).sum()\n",
    "                    \n",
    "    y = self.activate(conv + pm['b'], hconfig)\n",
    "    \n",
    "    return y, [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_conv_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    kh, kw, _, ychn = pm['k'].shape\n",
    "    \n",
    "    x_flat = get_ext_regions_for_conv(x, kh, kw)\n",
    "    k_flat = pm['k'].reshape([kh*kw*xchn, ychn])\n",
    "    conv_flat = np.matmul(x_flat, k_flat)\n",
    "    conv = conv_flat.reshape([mb_size, xh, xw, ychn])\n",
    "\n",
    "    y = self.activate(conv + pm['b'], hconfig)\n",
    "\n",
    "    if self.need_maps: self.maps.append(y)\n",
    "    \n",
    "    return y, [x_flat, k_flat, x, y]\n",
    "\n",
    "CnnBasicModel.forward_conv_layer = cnn_basic_forward_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_backprop_conv_layer(self, G_y, hconfig, pm, aux):\n",
    "    x_flat, k_flat, x, y = aux\n",
    "    \n",
    "    kh, kw, xchn, ychn = pm['k'].shape\n",
    "    mb_size, xh, xw, _ = G_y.shape\n",
    "    \n",
    "    G_conv = self.activate_derv(G_y, y, hconfig)\n",
    "\n",
    "    G_conv_flat = G_conv.reshape(mb_size*xh*xw, ychn)\n",
    "\n",
    "    g_conv_k_flat = x_flat.transpose()\n",
    "    g_conv_x_flat = k_flat.transpose()\n",
    "    \n",
    "    G_k_flat = np.matmul(g_conv_k_flat, G_conv_flat)\n",
    "    G_x_flat = np.matmul(G_conv_flat, g_conv_x_flat)\n",
    "    G_bias = np.sum(G_conv_flat, axis = 0)\n",
    "    \n",
    "    G_kernel = G_k_flat.reshape([kh, kw, xchn, ychn])\n",
    "    G_input = undo_ext_regions_for_conv(G_x_flat, x, kh, kw)\n",
    "    \n",
    "    self.update_param(pm, 'k', G_kernel)\n",
    "    self.update_param(pm, 'b', G_bias)\n",
    "    \n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.backprop_conv_layer = cnn_basic_backprop_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext_regions_for_conv(x, kh, kw):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "\n",
    "    regs = get_ext_regions(x, kh, kw, 0)\n",
    "    regs = regs.transpose([2, 0, 1, 3, 4, 5])\n",
    "    \n",
    "    return regs.reshape([mb_size*xh*xw, kh*kw*xchn])\n",
    "\n",
    "def get_ext_regions(x, kh, kw, fill):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "    \n",
    "    eh, ew = xh + kh - 1, xw + kw - 1\n",
    "    bh, bw = (kh-1)//2, (kw-1)//2\n",
    "\n",
    "    x_ext = np.zeros((mb_size, eh, ew, xchn), dtype = 'float32') + fill\n",
    "    x_ext[:, bh:bh + xh, bw:bw + xw, :] = x\n",
    "    \n",
    "    regs = np.zeros((xh, xw, mb_size*kh*kw*xchn), dtype = 'float32')\n",
    "\n",
    "    for r in range(xh):\n",
    "        for c in range(xw):\n",
    "            regs[r, c, :] = x_ext[:, r:r + kh, c:c + kw, :].flatten()\n",
    "\n",
    "    return regs.reshape([xh, xw, mb_size, kh, kw, xchn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_ext_regions_for_conv(regs, x, kh, kw):\n",
    "    mb_size, xh, xw, xchn = x.shape\n",
    "\n",
    "    regs = regs.reshape([mb_size, xh, xw, kh, kw, xchn])\n",
    "    regs = regs.transpose([1, 2, 0, 3, 4, 5])\n",
    "    \n",
    "    return undo_ext_regions(regs, kh, kw)\n",
    "\n",
    "def undo_ext_regions(regs, kh, kw):\n",
    "    xh, xw, mb_size, kh, kw, xchn = regs.shape\n",
    "    \n",
    "    eh, ew = xh + kh - 1, xw + kw - 1\n",
    "    bh, bw = (kh-1)//2, (kw-1)//2\n",
    "\n",
    "    gx_ext = np.zeros([mb_size, eh, ew, xchn], dtype = 'float32')\n",
    "\n",
    "    for r in range(xh):\n",
    "        for c in range(xw):\n",
    "            gx_ext[:, r:r + kh, c:c + kw, :] += regs[r, c]\n",
    "\n",
    "    return gx_ext[:, bh:bh + xh, bw:bw + xw, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_avg_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    yh, yw = xh // sh, xw // sw\n",
    "\n",
    "    x1 = x.reshape([mb_size, yh, sh, yw, sw, chn])\n",
    "    x2 = x1.transpose(0, 1, 3, 5, 2, 4)\n",
    "    x3 = x2.reshape([-1, sh*sw])\n",
    "    \n",
    "    y_flat = np.average(x3, 1)\n",
    "    y = y_flat.reshape([mb_size, yh, yw, chn])\n",
    "    \n",
    "    if self.need_maps: self.maps.append(y)\n",
    "\n",
    "    return y, None\n",
    "\n",
    "def cnn_basic_backprop_avg_layer(self, G_y, hconfig, pm, aux):\n",
    "    mb_size, yh, yw, chn = G_y.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    xh, xw = yh * sh, yw * sw\n",
    "    \n",
    "    gy_flat = G_y.flatten() / (sh * sw)\n",
    "\n",
    "    gx1 = np.zeros([mb_size*yh*yw*chn, sh*sw], dtype = 'float32')\n",
    "    for i in range(sh*sw):\n",
    "        gx1[:, i] = gy_flat\n",
    "    gx2 = gx1.reshape([mb_size, yh, yw, chn, sh, sw])\n",
    "    gx3 = gx2.transpose([0, 1, 4, 2, 5, 3])\n",
    "\n",
    "    G_input = gx3.reshape([mb_size, xh, xw, chn])\n",
    "        \n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.forward_avg_layer = cnn_basic_forward_avg_layer\n",
    "CnnBasicModel.backprop_avg_layer = cnn_basic_backprop_avg_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_forward_max_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    yh, yw = xh // sh, xw // sw\n",
    "\n",
    "    x1 = x.reshape([mb_size, yh, sh, yw, sw, chn])\n",
    "    x2 = x1.transpose(0, 1, 3, 5, 2, 4)\n",
    "    x3 = x2.reshape([-1, sh*sw])\n",
    "\n",
    "    idxs = np.argmax(x3, axis = 1)\n",
    "    y_flat = x3[np.arange(mb_size*yh*yw*chn), idxs]\n",
    "    y = y_flat.reshape([mb_size, yh, yw, chn])\n",
    "    \n",
    "    if self.need_maps: self.maps.append(y)\n",
    "\n",
    "    return y, idxs\n",
    "\n",
    "def cnn_basic_backprop_max_layer(self, G_y, hconfig, pm, aux):\n",
    "    idxs = aux\n",
    "    \n",
    "    mb_size, yh, yw, chn = G_y.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride')\n",
    "    xh, xw = yh * sh, yw * sw\n",
    "    \n",
    "    gy_flat = G_y.flatten()\n",
    "\n",
    "    gx1 = np.zeros([mb_size*yh*yw*chn, sh*sw], dtype = 'float32')\n",
    "    gx1[np.arange(mb_size*yh*yw*chn), idxs] = gy_flat[:]\n",
    "    gx2 = gx1.reshape([mb_size, yh, yw, chn, sh, sw])\n",
    "    gx3 = gx2.transpose([0, 1, 4, 2, 5, 3])\n",
    "\n",
    "    G_input = gx3.reshape([mb_size, xh, xw, chn])\n",
    "        \n",
    "    return G_input\n",
    "\n",
    "CnnBasicModel.forward_max_layer = cnn_basic_forward_max_layer\n",
    "CnnBasicModel.backprop_max_layer = cnn_basic_backprop_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_visualize(self, num):\n",
    "    print('Model {} Visualization'.format(self.name))\n",
    "    \n",
    "    self.need_maps = self.show_maps\n",
    "    self.maps = []\n",
    "\n",
    "    deX, deY = self.dataset.get_visualize_data(num)\n",
    "    est = self.get_estimate(deX)\n",
    "\n",
    "    if self.show_maps:\n",
    "        for kernel in self.kernels:\n",
    "            kh, kw, xchn, ychn = kernel.shape\n",
    "            grids = kernel.reshape([kh, kw, -1]).transpose(2, 0, 1)\n",
    "            draw_images_horz(grids[0:5, :, :])\n",
    "\n",
    "        for pmap in self.maps:\n",
    "            draw_images_horz(pmap[:, :, :, 0])\n",
    "        \n",
    "    self.dataset.visualize(deX, est, deY)\n",
    "\n",
    "    self.need_maps = False\n",
    "    self.maps = None\n",
    "\n",
    "CnnBasicModel.visualize = cnn_basic_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_basic_alloc_logic_full_layer(self, input_shape, hconfig):\n",
    "    input_cnt = 1\n",
    "    output_cnt = 1\n",
    "    \n",
    "    weight = np.array([[1]])\n",
    "\n",
    "    return {'w':weight}, [output_cnt]\n",
    "\n",
    "CnnBasicModel.alloc_logic_full_layer = cnn_basic_alloc_logic_full_layer\n",
    "\n",
    "def cnn_basic_forward_logic_full_layer(self, x, hconfig, pm):\n",
    "    if pm is None: return x, None\n",
    "    \n",
    "    x_org_shape = x.shape\n",
    "    \n",
    "    if len(x.shape) != 2:\n",
    "        mb_size = x.shape[0]\n",
    "        x = x.reshape([mb_size, -1])\n",
    "    #print(x.shape, pm['w'].shape,pm['b'].shape)\n",
    "    affine = np.matmul(x, pm['w'])\n",
    "    y = affine\n",
    "    \n",
    "    return y, [x, y, x_org_shape]\n",
    "\n",
    "CnnBasicModel.forward_logic_full_layer = cnn_basic_forward_logic_full_layer\n",
    "\n",
    "def cnn_basic_backprop_logic_full_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "\n",
    "    x, y, x_org_shape = aux\n",
    "\n",
    "    \n",
    "\n",
    "    return G_y.reshape(x_org_shape)\n",
    "\n",
    "CnnBasicModel.backprop_logic_full_layer = cnn_basic_backprop_logic_full_layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
